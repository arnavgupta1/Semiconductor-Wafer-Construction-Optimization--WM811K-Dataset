{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is code belonging to Arnav Gupta regarding his internship at Tech Mahindra. \n",
    "\n",
    "The project is about Semi-Conductor Wafer Construction Optimization. We are trying to identify which wafers are able to be sold to consumers and if they're not, what their failure is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and start sorting data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "wafer_data = pd.read_pickle(\"LSWMD.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wafer_data = wafer_data.drop(['waferIndex'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaigupta/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1603: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x, y, op)\n"
     ]
    }
   ],
   "source": [
    "noType = wafer_data[wafer_data.failureType.str.len() == 0]\n",
    "noneType = wafer_data[wafer_data.failureType == 'none']\n",
    "wafer_data = wafer_data[wafer_data.failureType.str.len() > 0]\n",
    "hasType = wafer_data[wafer_data.failureType != 'none']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import important libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.version\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort the data to only get a specific size and shape to pass into model and also split into training and test data**\n",
    "\n",
    "*This is before resizing and scaling the data so all of it can be used.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasType = hasType.reset_index(drop = True)\n",
    "currLen_train = []\n",
    "currLen_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "l = 0\n",
    "for i in range(len(hasType)):\n",
    "    if(len(hasType.waferMap[i]) == 26 and hasType.trianTestLabel[i] == 'Training' and len(hasType.waferMap[i][0]) == 26):\n",
    "        currLen_test.append(hasType.waferMap[i][:])\n",
    "        #y_test.append(hasType.failureType[i][:])\n",
    "    elif(len(hasType.waferMap[i]) == 26 and hasType.trianTestLabel[i] == 'Test' and len(hasType.waferMap[i][0]) == 26):\n",
    "        currLen_train.append(hasType.waferMap[i][:])\n",
    "       # y_train.append(hasType.failureType[i][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test out PIL library and resize random images to get an idea of how the library works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14549a0f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x72 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from numpy import array\n",
    "hasType = hasType.reset_index(drop = True)\n",
    "img = Image.fromarray(currLen_test[9])\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize=(2, 1))\n",
    "#ax[0].imshow(img)\n",
    "wpercent = (300/float(img.size[0]))\n",
    "hsize = int((float(img.size[1])*float(wpercent)))\n",
    "newimg = img.resize((500,500), Image.ANTIALIAS)\n",
    "newimg2 = img.resize((300,300))\n",
    "ax[0].imshow(newimg)\n",
    "ax[1].imshow(newimg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize all of the data to 75x75 and split into training and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasType = hasType.reset_index(drop = True)\n",
    "scaledimgs = [0]\n",
    "scaledtrain = [0]\n",
    "scaledtest = [0]\n",
    "\n",
    "traincount = 0\n",
    "testcount = 0\n",
    "for y in range(len(hasType)):\n",
    "    img = Image.fromarray(hasType.waferMap[y])\n",
    "    scimg = img.resize((75, 75), Image.ANTIALIAS)\n",
    "    arrayscimg = array(scimg)\n",
    "    scaledimgs.append(arrayscimg)\n",
    "\n",
    "scaledimgs = np.asarray(scaledimgs)\n",
    "\n",
    "y = 0\n",
    "scaledtrain.pop(0)\n",
    "for y in range(len(hasType)):\n",
    "    if(hasType.trianTestLabel[y] == 'Training'):\n",
    "        if(type(scaledimgs[y]) is not int):\n",
    "            scaledtrain.append(scaledimgs[y])\n",
    "        traincount += 1\n",
    "        y_train.append(hasType.failureType[y][:])\n",
    "    elif(hasType.trianTestLabel[y] == 'Test'):\n",
    "        scaledtest.append(scaledimgs[y])\n",
    "        testcount += 1\n",
    "        y_test.append(hasType.failureType[y][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert lists into arrays and get rid of empty data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledtest.pop(0)\n",
    "scaledtrain.pop(0)\n",
    "scaledtrain.pop(1)\n",
    "sctrainarray = np.asarray(scaledtrain)\n",
    "sctestarray = np.asarray(scaledtest)\n",
    "sctrainarray = np.delete(sctrainarray, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7894, 75, 75)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sctestarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sctestarray = sctestarray.reshape([7894, 75, 75, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Convolutional Neural Network to classify each of the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, input_shape = (75, 75, 1), kernel_size = (2, 2), strides = (2,2), activation = \"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
    "    keras.layers.Conv2D(64, kernel_size = (2, 2), strides = (2,2), activation = \"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
    "    keras.layers.Conv2D(128, kernel_size = (2, 2), strides = (2,2), activation = \"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(440575, activation='relu'),\n",
    "    keras.layers.Dense(8, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fy_train = np.vstack(y_train)\n",
    "fy_test = np.vstack(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot encode y arrays to be able to classify the images**\n",
    "\n",
    "*Models can't sort to strings, only arrays and integers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaigupta/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "ytrain_encoded = label_encoder.fit_transform(fy_train)\n",
    "test_encoded = label_encoder.fit_transform(fy_test)\n",
    "labels2 = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 16:07:40.258959 4523095488 deprecation.py:323] From /Users/ajaigupta/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7894 samples\n",
      "Epoch 1/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 1.9128 - accuracy: 0.2474\n",
      "Epoch 2/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.7120 - accuracy: 0.3354\n",
      "Epoch 3/50\n",
      "7894/7894 [==============================] - 46s 6ms/sample - loss: 1.6934 - accuracy: 0.3512\n",
      "Epoch 4/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.6811 - accuracy: 0.3512\n",
      "Epoch 5/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.6736 - accuracy: 0.3512\n",
      "Epoch 6/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.6419 - accuracy: 0.3581\n",
      "Epoch 7/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.5591 - accuracy: 0.4054\n",
      "Epoch 8/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.5062 - accuracy: 0.4204\n",
      "Epoch 9/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.4609 - accuracy: 0.4473\n",
      "Epoch 10/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.4323 - accuracy: 0.4596\n",
      "Epoch 11/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.4006 - accuracy: 0.4731\n",
      "Epoch 12/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.3853 - accuracy: 0.4726\n",
      "Epoch 13/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.3837 - accuracy: 0.4742\n",
      "Epoch 14/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.3556 - accuracy: 0.4825\n",
      "Epoch 15/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.3361 - accuracy: 0.4894\n",
      "Epoch 16/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.3188 - accuracy: 0.4965\n",
      "Epoch 17/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.3140 - accuracy: 0.5008\n",
      "Epoch 18/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.2878 - accuracy: 0.5013\n",
      "Epoch 19/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.2621 - accuracy: 0.5136\n",
      "Epoch 20/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.2628 - accuracy: 0.5168\n",
      "Epoch 21/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 1.2367 - accuracy: 0.5271\n",
      "Epoch 22/50\n",
      "7894/7894 [==============================] - 53s 7ms/sample - loss: 1.2236 - accuracy: 0.5322\n",
      "Epoch 23/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 1.1964 - accuracy: 0.5398\n",
      "Epoch 24/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.1816 - accuracy: 0.5395\n",
      "Epoch 25/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 1.1729 - accuracy: 0.5438\n",
      "Epoch 26/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.1692 - accuracy: 0.5528\n",
      "Epoch 27/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.1356 - accuracy: 0.5642\n",
      "Epoch 28/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.1089 - accuracy: 0.5707\n",
      "Epoch 29/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.0738 - accuracy: 0.5802\n",
      "Epoch 30/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.0639 - accuracy: 0.5854\n",
      "Epoch 31/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.0770 - accuracy: 0.5783\n",
      "Epoch 32/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.0474 - accuracy: 0.5960\n",
      "Epoch 33/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.0479 - accuracy: 0.6001\n",
      "Epoch 34/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 1.0174 - accuracy: 0.6062\n",
      "Epoch 35/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.9850 - accuracy: 0.6272\n",
      "Epoch 36/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.9470 - accuracy: 0.6410\n",
      "Epoch 37/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.9069 - accuracy: 0.6570\n",
      "Epoch 38/50\n",
      "7894/7894 [==============================] - 50s 6ms/sample - loss: 0.8683 - accuracy: 0.6690\n",
      "Epoch 39/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 0.8552 - accuracy: 0.6798\n",
      "Epoch 40/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 0.8875 - accuracy: 0.6554\n",
      "Epoch 41/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 0.8485 - accuracy: 0.6708\n",
      "Epoch 42/50\n",
      "7894/7894 [==============================] - 49s 6ms/sample - loss: 0.8050 - accuracy: 0.6950\n",
      "Epoch 43/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.7878 - accuracy: 0.7051\n",
      "Epoch 44/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.7713 - accuracy: 0.7015\n",
      "Epoch 45/50\n",
      "7894/7894 [==============================] - 47s 6ms/sample - loss: 0.7078 - accuracy: 0.7406\n",
      "Epoch 46/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.6891 - accuracy: 0.7466\n",
      "Epoch 47/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.7128 - accuracy: 0.7350\n",
      "Epoch 48/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.6794 - accuracy: 0.7450\n",
      "Epoch 49/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.6303 - accuracy: 0.7712\n",
      "Epoch 50/50\n",
      "7894/7894 [==============================] - 48s 6ms/sample - loss: 0.5897 - accuracy: 0.7869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11815acc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sctestarray, test_encoded, epochs=50, shuffle = True, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model to local machine and convert to TFLite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_file = \"wafermodel\"\n",
    "saved_model = tf.compat.v2.keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 1, name: conv2d_input ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-64c967aeebc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentative_dataset_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtflite_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_calibration_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m       result = self._calibrate_quantize_model(result, constants.FLOAT,\n\u001b[0;32m--> 396\u001b[0;31m                                               constants.FLOAT)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_calibrate_quantize_model\u001b[0;34m(self, result, inference_input_type, inference_output_type)\u001b[0m\n\u001b[1;32m    198\u001b[0m     return calibrate_quantize.calibrate_and_quantize(\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_input_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         inference_output_type, allow_float)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate_and_quantize\u001b[0;34m(self, dataset_gen, input_type, output_type, allow_float)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcalibration_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeedTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibration_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     return self._calibrator.QuantizeModel(\n\u001b[1;32m     77\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\u001b[0m in \u001b[0;36mFeedTensor\u001b[0;34m(self, input_value)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mFeedTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensorflow_lite_wrap_calibration_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrationWrapper_FeedTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mQuantizeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_py_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_py_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 1, name: conv2d_input "
     ]
    }
   ],
   "source": [
    "def representative_dataset_gen():\n",
    "    for _ in range(len(sctestarray)):\n",
    "    # Get sample input data as a numpy array in a method of your choosing.\n",
    "        yield [input]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(acurrLen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in range(len(sctrainarray)):\n",
    "    sctrainarray[g].reshape([75, 75, 1])\n",
    "    print(sctrainarray[g].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "newarray = []\n",
    "for b in range(len(sctrainarray)):\n",
    "    if(sctrainarray[b].shape == (75,75)):\n",
    "        newarray.append(sctrainarray[b])\n",
    "        count += 1\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sctestarray[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sctestarray.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
